---
title: "Model"
format: pdf
editor: visual
---

```{r}
library(tidyverse)
library(car)
library(MASS)
```

```{r}
data_train <- read.csv("data-train.csv")
data_test <- read.csv("data-test.csv")
```

##Data Cleaning

```{r}
# Change Fr and Re to categorical
data_train$Fr <- as.factor(data_train$Fr)
data_train$Re <- as.factor(data_train$Re)
```

#Moment 1

AIC lowest for log.
```{r}
adjusted_lm_moment1_unlog <- lm(R_moment_1 ~ poly(St, 2) * Re * Fr, data = data_train)
bc <- boxcox(adjusted_lm_moment1_unlog, lambda = seq(-2, 2, 0.1))

# Identify lambda with the highest log-likelihood
lambda_opt <- bc$x[which.max(bc$y)]
lambda_opt
```

# Final Model 1: (Linear)
```{r}
adjusted_lm_moment1 <- lm(log(R_moment_1) ~ poly(St,2) + Re * Fr, data = data_train) #Make St quadratic
summary(adjusted_lm_moment1)
par(mfrow = c(2,2))
plot(adjusted_lm_moment1)

#note: log response
#The Q–Q plot for the adjusted model (with quadratic St and interactions) indicates that while the central residuals are approximately normal, several extreme points deviate substantially from the expected line. This suggests the presence of heavy tails and influential cases, likely arising from extreme parameter settings. A log transformation of the response or a robust regression approach can mitigate these effects and improve model normality.

#note: ploy term
#Initial diagnostic plots revealed a systematic pattern in the residuals, suggesting that the effect of the Stokes number (St) on the mean cluster volume was nonlinear. To capture this curvature, we included a second-order polynomial term, poly(St, 2), which adds both linear and quadratic components of St to the model. This adjustment improves model flexibility while maintaining interpretability.

##TODO: Find which interaction effects are significant
```

```{r}
re_levels <- unique(newdata$Re)
par(mfrow = c(2,2))

for (r in re_levels) {
  sub <- subset(newdata, Re == r)

  p_r <- ggplot(sub, aes(x = St, y = pred, color = Fr, fill = Fr)) +
    geom_ribbon(aes(ymin = pred_lwr, ymax = pred_upr), alpha = 0.15, color = NA) +
    geom_line(size = 1.1) +
    labs(
      title = paste("Predicted mean cluster size vs St — Re =", r),
      x = "Stokes number (St)",
      y = "Predicted mean cluster size",
      color = "Froude (Fr)",
      fill  = "Froude (Fr)"
    ) +
    theme_minimal(base_size = 14) +
    theme(legend.position = "bottom")

  print(p_r)
  ggsave(paste0("pred_mean_St_Re", r, ".png"), p_r, width = 6, height = 4, dpi = 200)
}
```


#Moment 1 CV

```{r}
library(boot)

cv_results <- cv.glm(data_train, glm_moment1, K = 10)
cv_results$delta
```

Very small CV error. The model generalizes well within the sample (i.e., not just memorizing).

#Calculate Variance (Central Statistic 2)

```{r}
data_train$Var_empirical <- data_train$R_moment_2 - data_train$R_moment_1^2
```


#ANOVA to determine order of polynomial #??Should I include the interaction effect even though it's less interpretable? Should I do CV to determine?

```{r}
adjusted_lm_moment2 <- lm(log(Var_empirical)~ poly(St,2)+ Re*Fr, data = data_train)
lm_linear <- lm(log(Var_empirical) ~ St + Re*Fr, data = data_train)
anova(lm_linear, adjusted_lm_moment2)
```

optimal lambda close to 0 so log transformation is most appropriate.

```{r}
adjusted_lm_moment2_unlog <- lm(Var_empirical ~ poly(St, 2) + Re * Fr, data = data_train)
bc <- boxcox(adjusted_lm_moment2_unlog, lambda = seq(-2, 2, 0.1))

# Find the optimal lambda (λ) with the highest log-likelihood
lambda_opt <- bc$x[which.max(bc$y)]
lambda_opt
```
lambda close to 0

#Adjusted LM2 #should I add a spline to improve the Q-Q plot? (only 4 outliers)

```{r}
summary(adjusted_lm_moment2)
par(mfrow = c(2,2))
plot(adjusted_lm_moment2)
```

#Moment 2 CV

```{r}
set.seed(123)
K <- 10
folds <- sample(rep(1:K, length.out = nrow(data_train)))

rmse <- numeric(K)
for(k in 1:K){
  train_idx <- folds != k
  test_idx  <- folds == k

  fit <- lm(log(Var_empirical) ~ poly(St, 2) + Re *Fr, data=data_train[train_idx,])
  preds_log <- predict(fit, newdata=data_train[test_idx,])
  obs_log <- log(data_train$R_moment_2[test_idx])
  rmse[k] <- sqrt(mean((obs_log - preds_log)^2))
}
mean(rmse)
```

Very close to model's in-sample RSE = 1.73. This means there’s little to no overfitting — out-of-sample performance matches training performance very closely.

#FINAL: Moment 2 model
```{r}
# adds spline
library(mgcv)

# Fit a Gamma GAM with log link
gam_var <- gam(
  Var_empirical ~ s(St) + Re * Fr,
  data = data_train,
  family = Gamma(link = "log")
)

# View model summary
summary(gam_var)
par(mfrow = c(2, 2))
gam.check(gam_var)
plot(gam_var, pages = 1, shade = TRUE)
```

#CV w/ Spline

```{r}
set.seed(123)
K <- 10
folds <- sample(rep(1:K, length.out = nrow(data_train)))
rmse <- numeric(K)

for (k in 1:K) {
  train_idx <- folds != k
  test_idx  <- folds == k
  
  fit <- gam(
    Var_empirical ~ s(St) + Re * Fr,
    data = data_train[train_idx, ],
    family = Gamma(link = "log")
  )
  
  preds_log <- predict(fit, newdata = data_train[test_idx, ], type = "link")
  obs_log <- log(data_train$Var_empirical[test_idx])
  
  rmse[k] <- sqrt(mean((obs_log - preds_log)^2))
}

mean(rmse)
```

#Moment 3

```{r}
plot(lm_moment3)
```


```{r}
# --- Standardize Skewness ---
data_train <- data_train %>%
  mutate(
    Central_m2 = R_moment_2 - R_moment_1^2,
    Central_m3 = R_moment_3 - 3 * R_moment_2 * R_moment_1 + 2 * R_moment_1^3,
    Skew_Standardized = Central_m3 / (Central_m2^(3/2))
  ) %>%
  filter(!is.na(Skew_Standardized), is.finite(Skew_Standardized))

# --- Polynomial Comparison ---
lm_poly1 <- lm(Skew_Standardized ~ St + Re * Fr, data = data_train)
lm_poly2 <- lm(Skew_Standardized ~ poly(St, 2) + Re * Fr, data = data_train)
lm_poly3 <- lm(Skew_Standardized ~ poly(St, 3) + Re * Fr, data = data_train)
lm_poly4 <- lm(Skew_Standardized ~ poly(St, 4) + Re * Fr, data = data_train)
anova(lm_poly1, lm_poly2, lm_poly3, lm_poly4)

# Difference in RMSE between poly 2 and 3 for lm_raw is marginal, so we’re sticking with 2

```

#FINAL: Model 3 (MARK)
```{r}
library(mgcv)
library(caret)

set.seed(123)

# Make sure data_train exists and contains Skew_Standardized, St, Re, Fr
# create 5 folds (list of indices)
folds <- createFolds(data_train$Skew_Standardized, k = 5, list = TRUE)

cv_mse_per_fold <- sapply(seq_along(folds), function(i) {
  test_idx  <- folds[[i]]
  train_idx <- setdiff(seq_len(nrow(data_train)), test_idx)

  train <- data_train[train_idx, , drop = FALSE]
  test  <- data_train[test_idx,  , drop = FALSE]

  # Fit GAM on the training fold
  fit <- mgcv::gam(Skew_Standardized ~ s(St) + Re * Fr, data = train, method = "REML")

  # Predict on the test fold
  preds <- predict(fit, newdata = test, type = "response")

  # MSE for this fold (ignore any NA predictions)
  mean((test$Skew_Standardized - preds)^2, na.rm = TRUE)
})

# Report results
cv_results <- data.frame(Fold = seq_along(cv_mse_per_fold), MSE = cv_mse_per_fold)
print(cv_results)
cat("Mean 5-fold CV MSE (gam_auto):", mean(cv_mse_per_fold), "\n")

# Fit final GAM on the full training set
gam_auto_gamma <- gam(Skew_Standardized ~ s(St) + Re *Fr,
                      data = data_train, family = Gamma(link = "log"))
# Summary and diagnostics
summary(gam_auto_gamma)

# GAM diagnostic checks and smooth plot(s)
par(mfrow = c(2, 2))
gam.check(gam_auto_gamma)
par(mfrow = c(1, 1))
plot(gam_auto_gamma, shade = TRUE, pages = 1)
```
```{r}
library(mgcv)
library(ggplot2)
library(dplyr)
library(tidyr)

# TARGETS (from your example)
target_Re <- c(90, 224, 398)
target_Fr <- c(0.052, 0.3, Inf)  # Inf interpreted as max(observed Fr)

# Ensure numeric representations for matching
data_Re_num <- suppressWarnings(as.numeric(as.character(data_train$Re)))
data_Fr_num <- suppressWarnings(as.numeric(as.character(data_train$Fr)))

# If conversion gives NA, treat Re/Fr as factors
Re_is_numeric <- !all(is.na(data_Re_num))
Fr_is_numeric <- !all(is.na(data_Fr_num))

# Get the actual Re values to plot (closest observed values)
if (Re_is_numeric) {
  observed_Re <- unique(data_Re_num)
  chosen_Re <- sapply(target_Re, function(t) {
    # for targets that are NA (Inf), choose max
    if (is.infinite(t)) max(observed_Re, na.rm = TRUE) else observed_Re[which.min(abs(observed_Re - t))]
  })
} else {
  # If Re is factor: pick top 3 most common levels or the ones matching names if numeric-like
  observed_Re <- levels(as.factor(data_train$Re))
  # just take up to 3 most frequent levels
  chosen_Re <- names(sort(table(data_train$Re), decreasing = TRUE))[1:3]
}

# Get actual Fr values to plot (closest observed values)
if (Fr_is_numeric) {
  observed_Fr <- unique(data_Fr_num)
  chosen_Fr <- sapply(target_Fr, function(t) {
    if (is.infinite(t)) max(observed_Fr, na.rm = TRUE) else observed_Fr[which.min(abs(observed_Fr - t))]
  })
} else {
  observed_Fr <- levels(as.factor(data_train$Fr))
  chosen_Fr <- names(sort(table(data_train$Fr), decreasing = TRUE))[1:3]
}

# Create St grid inside observed range (no extrapolation)
St_min <- min(data_train$St, na.rm = TRUE)
St_max <- max(data_train$St, na.rm = TRUE)
St_grid <- seq(St_min, St_max, length.out = 180)

# Build prediction grid
pred_grid <- expand.grid(
  St = St_grid,
  Re = chosen_Re,
  Fr = chosen_Fr,
  stringsAsFactors = FALSE
) %>% as_tibble()

# Make sure Re/Fr in pred_grid have the same type/levels as in training data
if (!Re_is_numeric) {
  pred_grid$Re <- factor(pred_grid$Re, levels = levels(as.factor(data_train$Re)))
} else {
  pred_grid$Re <- as.numeric(as.character(pred_grid$Re))
}

if (!Fr_is_numeric) {
  pred_grid$Fr <- factor(pred_grid$Fr, levels = levels(as.factor(data_train$Fr)))
} else {
  pred_grid$Fr <- as.numeric(as.character(pred_grid$Fr))
}

# Safety checks
if (any(is.na(pred_grid$Re))) stop("Re levels in prediction grid do not match training data.")
if (any(is.na(pred_grid$Fr))) stop("Fr values in prediction grid do not match training data.")

# Predict on link scale and get se.fit
pred_link <- predict(gam_auto_gamma, newdata = pred_grid, type = "link", se.fit = TRUE)
pred_grid <- pred_grid %>%
  mutate(
    fit_link = pred_link$fit,
    se_link  = pred_link$se.fit,
    fit      = exp(fit_link),
    lower    = exp(fit_link - 2 * se_link),
    upper    = exp(fit_link + 2 * se_link)
  )

# Make plotting labels — show numeric rounding for panels
pred_grid <- pred_grid %>%
  mutate(
    Re_label = if (Re_is_numeric) paste0("Re = ", round(as.numeric(Re), 0)) else paste0("Re = ", as.character(Re)),
    Fr_label = if (Fr_is_numeric) {
      # pretty label with Inf -> "Inf" if matches max
      ifelse(as.numeric(Fr) == max(as.numeric(data_train$Fr), na.rm = TRUE), "Fr = Inf", paste0("Fr = ", Fr))
    } else {
      paste0("Fr = ", as.character(Fr))
    }
  )

# Overlay data points (coerced to plotting labels)
data_points <- data_train %>%
  mutate(
    Re_label = if (Re_is_numeric) paste0("Re = ", round(as.numeric(as.character(Re)), 0)) else paste0("Re = ", as.character(Re)),
    Fr_label = if (Fr_is_numeric) {
      ifelse(as.numeric(as.character(Fr)) == max(as.numeric(as.character(data_train$Fr)), na.rm = TRUE),
             "Fr = Inf", paste0("Fr = ", Fr))
    } else {
      paste0("Fr = ", as.character(Fr))
    }
  )

# Convert Fr_label to factor and set panel order matching chosen_Fr
fr_levels <- unique(pred_grid$Fr_label)
pred_grid$Fr_label <- factor(pred_grid$Fr_label, levels = fr_levels)
data_points$Fr_label <- factor(data_points$Fr_label, levels = fr_levels)

# Plot: facets by Fr_label, lines colored by Re_label, y on log10 scale
p <- ggplot(pred_grid, aes(x = St, y = fit, color = Re_label, group = Re_label)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = Re_label), alpha = 0.12, color = NA, show.legend = FALSE) +
  geom_line(size = 1) +
  geom_point(data = data_points, aes(x = St, y = Skew_Standardized), inherit.aes = FALSE,
             color = "grey40", alpha = 0.45, size = 1.2) +
  facet_wrap(~ Fr_label, nrow = 1, scales = "fixed") +
  scale_y_log10() +
  labs(
    x = "St (Stokes number)",
    y = "Predicted Skew (log scale)",
    color = "Re",
    title = "Predicted skew vs St — panels by Fr, lines by Re",
    subtitle = "Model: GAM (Gamma, log link). Shaded = ±2·SE (back-transformed)"
  ) +
  theme_minimal(base_size = 13) +
  theme(legend.position = "bottom", plot.title = element_text(face = "bold"))

print(p)
```

#Andy
```{r}
plot(lm_moment4)
```

```{r}
adjusted_lm_moment4 <- lm(R_moment_4 ~ poly(St, 2) * Re * Fr, data = data_train) #Make St quadratic
summary(adjusted_lm_moment4)
plot(adjusted_lm_moment4)
```

```{r}
# add columns to training data
data_train <- data_train %>%
  mutate(
    Central_m4 = R_moment_4 - 4 * R_moment_3 * R_moment_1 +
                 6 * R_moment_2 * R_moment_1^2 - 3 * R_moment_1^4,
    Kurt_Standardized = Central_m4 / (Central_m2^2),
  ) %>%
  filter(!is.na(Kurt_Standardized), is.finite(Kurt_Standardized))
```

```{r}
# CV on a bunch of different models
set.seed(123)
cv_folds <- createFolds(data_train$Kurt_Standardized, k = 5)

cv_error_log <- function(fit_func) {
  mean(sapply(cv_folds, function(idx) {
    train <- data_train[-idx, ]
    test  <- data_train[idx, ]
    model <- fit_func(train)
    preds_log <- predict(model, newdata = test)
    preds_raw <- exp(preds_log)
    mean((test$Kurt_Standardized - preds_raw)^2, na.rm = TRUE)
  }))
}

cv_error_raw <- function(fit_func) {
  mean(sapply(cv_folds, function(idx) {
    train <- data_train[-idx, ]
    test  <- data_train[idx, ]
    model <- fit_func(train)
    preds <- predict(model, newdata = test)
    mean((test$Kurt_Standardized - preds)^2, na.rm = TRUE)
  }))
}

linear <- function(d) lm(Kurt_Standardized ~ St + Re + Fr, data = d)
simple <- function(d) lm(Kurt_Standardized ~ poly(St, 2) + Re * Fr, data = d)
interact <- function(d) lm(Kurt_Standardized ~ poly(St, 2) * Re + Re * Fr, data = d)
transform <- function(d) lm(log(Kurt_Standardized) ~ poly(St, 2) * Re + Re * Fr, data = d)
spline <- function(d) lm(Kurt_Standardized ~ ns(St, df = 4) + Re * Fr, data = d)

cv_error_raw(linear)
cv_error_raw(simple)
cv_error_raw(interact)
cv_error_log(transform)
cv_error_raw(spline)



```


```{r}
bc <- boxcox(model4, lambda = seq(-2, 2, 0.1))

# Extract lambda (λ) with the maximum log-likelihood
lambda_opt <- bc$x[which.max(bc$y)]
lambda_opt
```
lambda close to 0: should have log transformation.

#FINAL: moment 4 model
```{r}
# "simple" model from above performed best during CV
model4 <- lm(log(Kurt_Standardized) ~ poly(St, 2) + Re * Fr, data = data_train)

summary(model4)
par(mfrow = c(2, 2))
plot(model4)

# Residuals vs Fitted & Scale-Location plot look a bit sus, tried adding spline but it didnt help
```
```{r}
plot(gam_model, select = 1, shade = TRUE, main = "Effect of St on Variability (σ)")
```

