---
title: "Model"
format: pdf
editor: visual
---

```{r}
library(tidyverse)
library(car)
```

```{r}
data_train <- read.csv("data-train.csv")
data_test <- read.csv("data-test.csv")
```

##Data Cleaning

```{r}
# Change Fr and Re to categorical
data_train$Fr <- as.factor(data_train$Fr)
data_train$Re <- as.factor(data_train$Re)

```

## Initial linear models

```{r}
lm_moment1 <- lm(R_moment_1 ~ St + Re + Fr, data = data_train)
lm_moment2 <- lm(R_moment_2 ~ St + Re + Fr, data = data_train)
lm_moment3 <- lm(R_moment_3 ~ St + Re + Fr, data = data_train)
lm_moment4 <- lm(R_moment_4 ~ St + Re + Fr, data = data_train)

summary(lm_moment1)
summary(lm_moment2)
summary(lm_moment3)
summary(lm_moment4)

```
# Plots for Linear Model 1 (First Moment)
```{r}
par(mfrow = c(2,2))
plot(lm_moment1)
```
# Adjusted Linear Model 1 
```{r}
adjusted_lm_moment1 <- lm(log(R_moment_1) ~ poly(St, 2) * Re * Fr, data = data_train) #Make St quadratic
summary(adjusted_lm_moment1)
par(mfrow = c(2,2))
plot(adjusted_lm_moment1)

#note: log response
#The Qâ€“Q plot for the adjusted model (with quadratic St and interactions) indicates that while the central residuals are approximately normal, several extreme points deviate substantially from the expected line. This suggests the presence of heavy tails and influential cases, likely arising from extreme parameter settings. A log transformation of the response or a robust regression approach can mitigate these effects and improve model normality.

#note: ploy term
#Initial diagnostic plots revealed a systematic pattern in the residuals, suggesting that the effect of the Stokes number (St) on the mean cluster volume was nonlinear. To capture this curvature, we included a second-order polynomial term, poly(St, 2), which adds both linear and quadratic components of St to the model. This adjustment improves model flexibility while maintaining interpretability.

##TODO: Find which interaction effects are significant
```

#remove insignificant interaction effects

```{r}
adjusted_lm_moment1 <- lm(log(R_moment_1) ~ poly(St, 2) * Fr + Re:Fr, data = data_train) #Make St quadratic
summary(adjusted_lm_moment1)
par(mfrow = c(2,2))
plot(adjusted_lm_moment1)
```

# remove 2nd level of polynomial interaction (MAY NOT BE NECESSARY?)
```{r}
St_poly <- poly(data_train$St, 2)
data_train$St1 <- St_poly[,1]   
data_train$St2 <- St_poly[,2]  

simple_adj_lm_moment1 <- lm(log(R_moment_1) ~ poly(St, 2) * Fr + Re:Fr, data = data_train)
summary(simple_adj_lm_moment1)
par(mfrow = c(2,2))
plot(simple_adj_lm_moment1)

AIC(simple_adj_lm_moment1, adjusted_lm_moment1); BIC(simple_adj_lm_moment1, adjusted_lm_moment1)
```
# GLM
#!!! NEED TO USE ANOVA TO COMPARE VARIANCS of St Polynomials
```{r}
glm_moment1 <-glm(R_moment_1 ~ St*Fr + Re, data = data_train, family = Gamma(link = "log"))
summary(glm_moment1)
par(mfrow = c(2,2))
plot(glm_moment1)
```
```{r}
library(boot)

# 10-fold CV for your Gamma model
cv_results <- cv.glm(data_train, glm_moment1, K = 10)
cv_results$delta
```
Very small CV error. The model generalizes well within the sample (i.e., not just memorizing).
# LM 2 Plot
```{r}
par(mfrow = c(2,2))
plot(lm_moment2)
```

#GLM2 Starting point
```{r}
glm_moment2 <- glm(R_moment_2 ~ St1 + St2 + Re * Fr,
                   data = data_train,
                   family = Gamma(link = "log"))
summary(glm_moment2)
par(mfrow = c(2,2))
plot(glm_moment2)
```


```{r}
lm_log <- lm(log(R_moment_2) ~ St1 + St2 + Re * Fr, data = data_train)
summary(lm_log)
plot(lm_log)
```


```{r}
plot(lm_moment3)
```

```{r}
adjusted_lm_moment3 <- lm(R_moment_3 ~ poly(St, 2) * Re * Fr, data = data_train) #Make St quadratic
summary(adjusted_lm_moment3)
plot(adjusted_lm_moment3)
```

```{r}
plot(lm_moment4)
```

```{r}
adjusted_lm_moment4 <- lm(R_moment_4 ~ poly(St, 2) * Re * Fr, data = data_train) #Make St quadratic
summary(adjusted_lm_moment4)
plot(adjusted_lm_moment4)
```
